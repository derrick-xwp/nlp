from __future__ import division

import numpy as np
import logging
import csv
import time as tm
import pandas as pd
import sys
import operator
import math

import numpy as np


def cos_similarity(v1, v2):
    prod = np.dot(v1, v2)
    len1 = math.sqrt(np.dot(v1, v1))
    len2 = math.sqrt(np.dot(v2, v2))
    return prod / (len1 * len2)


def update_progress(progress):
    barLength = 10 # Modify this to change the length of the progress bar
    status = ""
    if isinstance(progress, int):
        progress = float(progress)
    if not isinstance(progress, float):
        progress = 0
        status = "error: progress var must be float\r\n"
    if progress < 0:
        progress = 0
        status = "Halt...\r\n"
    if progress >= 1:
        progress = 1
        status = "Done...\r\n"
    block = int(round(barLength*progress))
    text = "\rPercent: [{0}] {1}% {2}".format( "#"*block + "-"*(barLength-block), progress*100, status)
    sys.stdout.write(text)
    sys.stdout.flush()

datetime = tm.localtime()
date = '{0:}-{1:}-{2:}'.format(datetime.tm_mon, datetime.tm_mday, datetime.tm_year)
time = '{0:}:{1:}:{2:}'.format(datetime.tm_hour, datetime.tm_min, datetime.tm_sec)
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

def file_len(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    return i + 1

def get_loadings(dic_name, agg_doc_vecs_path, agg_dic_vecs_path, out_path, num_features, words_without_value, words, delimiter='\t'):
    '''

    :param agg_doc_vecs_path: Path to distributed representations of documents
    :param agg_dic_vecs_path: Path to distributed representations of dictionaries
    :param out_path: Path to write to
    :param num_features: Number of dimensions in distributed representations
    :param delimiter: Delimiter to use
    :return:
    '''
    """Get loadings between each document vector in agg-doc_vecs_path and each dictionary dimension in
    agg_dic_vecs_path"""
    n_docs = float(file_len(agg_doc_vecs_path))
    prog_counter = 0
    counter = 0
    dic_vecs = pd.read_csv(agg_dic_vecs_path, sep=delimiter)
    dic_vecs = dic_vecs.to_dict(orient='list')
    nan_counter = {'ID': [], 'count': 0}

    with open(agg_doc_vecs_path, 'r') as doc_vecs, open(out_path, 'w') as out_file:

        doc_vecs_reader = csv.reader(doc_vecs, delimiter='\t')
        next(doc_vecs_reader)

        writer = csv.writer(out_file, delimiter='\t')
        fieldnames_out = ['ID'] + list(dic_vecs.keys())

        writer.writerow(fieldnames_out)
        rows = []
        for doc_vec in doc_vecs_reader:
            if 'nan' in doc_vec:
                nan_counter['count'] += 1
                nan_counter['ID'].append(doc_vec[0])
                if int(doc_vec[0]) == 1:
                    continue
                rows.append(0)
                if words[int(doc_vec[0])-2] not in words_without_value:
                    words_without_value.append(words[int(doc_vec[0])-2])
            else:
                prog_counter += 1
                counter += 1
                doc_id = doc_vec[0]
                out_row = []

                for k in iter(dic_vecs.keys()):
                    doc_vec = [np.float64(x) for x in doc_vec[-num_features:]]
                    dic_similarity = cos_similarity(doc_vec, dic_vecs[k])
                    out_row.append(float(dic_similarity))

                writer.writerow(out_row)
                rows.append(out_row)

                if prog_counter >= 0.01 * n_docs:
                    prog_counter = 0
                    update_progress(counter / (n_docs - 1))

        print('Failed to calculate {0} loadings due to missing values.'.format(nan_counter['count']))
        print('IDs for documents with missing values:\n\n', nan_counter['ID'])

        return rows
