from __future__ import division

__author__ = 'Joe Hoover'

import sys
import numpy as np
import os
import collections
import csv

import pandas as pd


import os
import collections
import pandas as pd

def file_len(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    return i + 1

def update_progress(progress):
    barLength = 10 # Modify this to change the length of the progress bar
    status = ""
    if isinstance(progress, int):
        progress = float(progress)
    if not isinstance(progress, float):
        progress = 0
        status = "error: progress var must be float\r\n"
    if progress < 0:
        progress = 0
        status = "Halt...\r\n"
    if progress >= 1:
        progress = 1
        status = "Done...\r\n"
    block = int(round(barLength*progress))
    text = "\rPercent: [{0}] {1}% {2}".format( "#"*block + "-"*(barLength-block), progress*100, status)
    sys.stdout.write(text)
    sys.stdout.flush()
def get_files(input_path):
    """

    :param input_path: Directory containing term file(s)
    :return: A dictionary of file paths. Keys are file names and are used to
    name dimensions represented in other functions
    """


    path_info = collections.OrderedDict()

    if os.path.isfile(input_path):
        f_name = input_path.split('/')[-1]
        path_info[f_name] = input_path

    else:
        for f in os.listdir(input_path):

            if os.path.isfile(os.path.join(input_path, f)) and not f.startswith('.'):
                path_info[f] = os.path.join(input_path, f)

            elif os.path.isdir(os.path.join(input_path, f)):
                sub_path = os.path.join(input_path, f)
                get_files(input_path=sub_path)

    return path_info


def terms_from_txt(input_path):
    '''

    :param input_path: Path to directory containing term file(s)
    :return: A dictionary with dimension names as keys and words as values.
    '''
    dic_terms_out = collections.OrderedDict()
    path_info = get_files(input_path=input_path)

    for k in path_info.iterkeys():
        current_file = path_info[k]

        with open(current_file, 'rb') as dict_file:
            dict_terms = list()
            dict_terms = dict_file.read()
            dict_terms = dict_terms.lower()
            dict_terms = dict_terms.split()
            dic_terms_out[k] = dict_terms

    return dic_terms_out


def terms_from_liwc(input_path):
    '''
    Read LIWC format dictionary into Python dictionary format

    :param input_path: Path to LIWC dictionary
    :return: A dictionary with dimension names as keys and words as values.
    '''

    get_dims = 0
    with open(input_path, 'r') as document:

        out_dic = collections.defaultdict(list)
        codes = {}

        for line in document:
            line = line.split()
            if not line:  # empty line?
                continue

            if '%' in line:
                get_dims += 1

            if '%' not in line and get_dims == 1:
                codes[line[0]] = line[1]

            if get_dims == 2 and '%' not in line:
                words = []
                dims = []
                for el in line:
                    try:
                        int(el)
                    except ValueError:
                        words.append(el)

                    if el in codes.keys():

                        dims.append(el)

                for dim in dims:
                    out_dic[codes[dim]].extend(words)

    print("Number of dimensions registered: {0}".format(len(out_dic.keys())))
    print("Number of words registered: {0}".format(sum([len(i) for i in out_dic.values()])))
    return (out_dic)



# Read CSV file of dictionary terms into Python dictionary. Column names should represent the dimension associated
# with the words in the column

def terms_from_csv(input_path, delimiter):
    '''
    Read CSV file of dictionary terms into Python dictionary. Column names should represent the dimension associated
    with the words in the column.
    :param input_path: Path to csv
    :param delimiter: delimiter used in csv file
    :return: A dictionary with csv column names as keys and words as values.
    '''
    import pandas as pd
    dic_terms = pd.read_csv(input_path, delimiter)
    dic_terms.to_dict(orient='list')

    return dic_terms

def terms_to_csv(terms_dic, output_path, delimiter='\t'):
    '''
    Write Python dictionary of terms to csv with column names as dimensions and cells as words

    :param terms_dic: Dictionary of terms where keys are dimension names and values are terms
    :param output_path: Path to output file
    :param delimiter: Delimiter to use for output
    :return: None.
    '''

    terms_dic = pd.DataFrame.from_dict(terms_dic, orient='columns')
    terms_dic.to_csv(output_path, sep=delimiter, index=False)

def make_agg_vec(words, model, num_features, model_word_set, filter_out=[]):
    """Create aggregate representation of list of words"""

    feature_vec = np.zeros((num_features,), dtype="float32")

    nwords = 0.

    for word in words:
        if word not in filter_out:
            if word in model_word_set:
                nwords += 1
                feature_vec = np.add(feature_vec, model[word])

    avg_feature_vec = feature_vec / nwords

    return avg_feature_vec


def dic_vecs(dic_terms, model, num_features, model_word_set, filter_out=[]):
    '''

    :param dic_terms: Dictionary where keys are dimension names and values are terms.
    :param model: word2vec model
    :param num_features: Number of dimensions in word2vec model
    :param model_word_set: Set of unique words in word2vec model
    :param filter_out: Words to exclude from aggregation
    :return: A dictionary where keys are dimension names and values the latent semantic space
     representing that dimension. len(values) will equal num_features.
    '''
    agg_dic_vecs = collections.OrderedDict()
    for k in iter(dic_terms.keys()):
        agg_dic_vecs[k] = make_agg_vec(dic_terms[k], model = model, num_features = num_features,
                                         model_word_set = model_word_set, filter_out = filter_out)
    return agg_dic_vecs

def write_dic_vecs(dic_vecs, output_path, delimiter='\t'):
    '''

    :param dic_vecs: Dictionary of term dictionary representations (e.g. as created by dic_vecs()
    :param output_path: File to write to
    :param delimiter: Delimiter for column sep in output file
    :return: None
    '''
    dic_vecs = pd.DataFrame.from_dict(dic_vecs, orient='columns')
    dic_vecs.to_csv(output_path, sep=delimiter, index=False)


def doc_vecs_from_csv(input_path, output_path, model, num_features, model_word_set, text_col, delimiter, filter_out=[],
                  quotechar=None,
                  id_col=False, header=True):
    """
    Create a distributed representation of each document in a column of documents
    contained in the input file. These representations are written to the file
    specified by the 'output_path' parameter.

    :param input_path: Path to csv containing text to be represented
    :param output_path: Path to file where results should be written
    :param model: word2vec model to use for representation
    :param num_features: Number of dimensions in word2vec model
    :param model_word_set: Set of unique words in word2vec model
    :param text_col: Column containing text to be represented. This can either be a column name or an
    integer representing the column position. Note that column indices begin at 0.
    :param filter_out: Words to exclude from representations
    :param delimiter: Delimiter used to separate columns in the input file
    :param quotechar: If quote character is used to indicate text in the input file,
    specify what character is used.
    :param id_col: If an ID column is included in the input file, specify
    the column it is located in via column name or position. If no ID column is available,
     this should be set to False. When id_col==False, a sequence of range(1:len(text_col))
     will be generated and each representation will be
    associated with a unique integer that corresponds to the row order in the original file.
    :param header: boolean indicating whether the input file contains a header (True) or not (False).
    :return: None. This function iteratively writes each document representation to file, no
    object is returned.
    """

    with open(input_path, 'r') as docs_file, open(output_path, 'w') as out_file:

        docs = csv.reader(docs_file, delimiter=delimiter, quotechar=quotechar)

        if header is True:
            header = docs.next()
            print(header)

            if id_col is not False:
                try:
                    id_col = header.index(id_col)
                except ValueError:
                    try:
                        id_col = int(id_col)
                    except ValueError:
                        print("ValueError: Column '{0}' not found, please make sure that the name or index was correctly listed".format(id_col))

            try:
                print(text_col)
                text_col = header.index(text_col)
            except ValueError:
                try:
                    text_col = int(text_col)
                except ValueError:
                    print("ValueError: Column '{0}' not found, please make sure that the name or index was correctly listed".format(text_col))

        if header is False:
            if id_col is not False:
                try:
                    id_col = int(id_col)
                except ValueError:
                    print("ValueError: Column '{0}' not found, please make sure that the index was correctly listed".format(id_col))

            try:
                text_col = int(text_col)
            except ValueError:
                print("ValueError: Column '{0}' not found, please make sure that the index was correctly listed".format(text_col))

        fieldnames = ['ID'] + [fnum for fnum in range(1, num_features + 1)]
        writer = csv.writer(out_file, delimiter=delimiter, quotechar=quotechar)
        #writer.writerow(bytes(''.join(fieldnames), encoding = "utf8"))
        writer.writerow(fieldnames)

        n_lines = float(file_len(input_path))
        print(n_lines)
        n_na = 0

        print('Generating aggregate distributed representations of', n_lines, 'texts.')
        update_progress(0 / (n_lines - 1))

        prog_counter = 0
        counter = 0

        if id_col is False:
            cur_id = 0

            for row in docs:
                try:
                    cur_id += 1
                    prog_counter += 1
                    counter += 1

                    doc = row[text_col].split()
                    cur_agg_vec = make_agg_vec(words=doc, model=model, num_features=num_features, model_word_set=model_word_set, filter_out=[])
                    writer.writerow([cur_id] + list(cur_agg_vec))

                    if prog_counter >= 0.05 * n_lines:
                        prog_counter = 0
                        update_progress(counter / (n_lines - 1))

                except IndexError:
                    n_na += 1
                    pass

        elif id_col is not False:
            for row in docs:
                prog_counter += 1
                counter += 1
                
                doc = row[text_col].split()
                cur_agg_vec = make_agg_vec(words=doc, model=model, num_features=num_features, model_word_set=model_word_set, filter_out = [])

                writer.writerow([row[id_col]] + list(cur_agg_vec))

                if prog_counter >= 0.05 * n_lines:
                    prog_counter = 0
                    update_progress(counter / (n_lines - 1))

            print("\nFinished calculating aggregate document representations", "\nNumber NA:", n_na)



def doc_vecs_from_txt(input_path, output_path, num_features, model, model_word_set, delimiter='\t', filter_out = []):
    '''

    :param input_path: Path to text file(s) containing texts to be represented.
    This can be a single file or a directory containing multiple files.
    :param output_path:  Path to file where results should be written
    :param num_features: Number of dimensions in word2vec model
    :param model: word2vec model to use for representation
    :param model_word_set: Set of unique words in word2vec model
    :param filter_out: words to be excluded from the representation from
    :return: None. None. This function iteratively writes each document representation to file, no
    object is returned.
    '''

    path_info = get_files(input_path=input_path)

    with open(output_path, 'wb') as out_file:

        fieldnames = ['ID'] + [unicode(fnum) for fnum in range(1, num_features + 1)]
        writer = csv.writer(out_file, delimiter=delimiter)
        writer.writerow(fieldnames)

        for input_path in path_info.itervalues():

            with open(input_path, 'rb') as docs:

                n_lines = float(file_len(input_path))

                print('Generating aggregate distributed representations of', n_lines, 'texts.')
                update_progress(0 / (n_lines - 1))

                prog_counter = 0
                counter = 0

                cur_id = 0
                n_na = 0

                for row in docs:

                    try:
                        cur_id += 1
                        prog_counter += 1
                        counter += 1
                        row = row[0].split()
                        cur_agg_vec = make_agg_vec(words=row, model=model, num_features=num_features,
                                                   model_word_set=model_word_set, filter_out=[])
                        writer.writerow([cur_id] + list(cur_agg_vec))

                        if prog_counter >= 0.05 * n_lines:
                            prog_counter = 0
                            update_progress(counter / (n_lines - 1))

                    except IndexError:

                        n_na += 1
                        pass
                print("\nFinished calculating aggregate document representations", "\nNumber of NA:", n_na)



